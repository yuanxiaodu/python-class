{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Inception at 0x23d6921fd90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Inception(layers.Layer):\n",
    "    def __init__(self, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        self.branch1 = layers.Conv2D(ch1x1, kernel_size=1, activation=\"relu\")\n",
    "\n",
    "        self.branch2 = Sequential([\n",
    "            layers.Conv2D(ch3x3red, kernel_size=1, activation=\"relu\"),\n",
    "            layers.Conv2D(ch3x3, kernel_size=3, padding=\"SAME\", activation=\"relu\")])      # output_size= input_size\n",
    "\n",
    "        self.branch3 = Sequential([\n",
    "            layers.Conv2D(ch5x5red, kernel_size=1, activation=\"relu\"),\n",
    "            layers.Conv2D(ch5x5, kernel_size=5, padding=\"SAME\", activation=\"relu\")])      # output_size= input_size\n",
    "\n",
    "        self.branch4 = Sequential([\n",
    "            layers.MaxPool2D(pool_size=3, strides=1, padding=\"SAME\"),  # caution: default strides==pool_size\n",
    "            layers.Conv2D(pool_proj, kernel_size=1, activation=\"relu\")])                  # output_size= input_size\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        branch1 = self.branch1(inputs)\n",
    "        branch2 = self.branch2(inputs)\n",
    "        branch3 = self.branch3(inputs)\n",
    "        branch4 = self.branch4(inputs)\n",
    "        outputs = layers.concatenate([branch1, branch2, branch3, branch4])\n",
    "        return outputs\n",
    "Inception(192, 96, 208, 16, 48, 64, name=\"inception_4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 112, 112, 64)      9472      \n",
      "_________________________________________________________________\n",
      "maxpool_1 (MaxPooling2D)     (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 64)        4160      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 192)       110784    \n",
      "_________________________________________________________________\n",
      "maxpool_2 (MaxPooling2D)     (None, 28, 28, 192)       0         \n",
      "_________________________________________________________________\n",
      "inception_3a (Inception)     (None, 28, 28, 256)       163696    \n",
      "_________________________________________________________________\n",
      "inception_3b (Inception)     (None, 28, 28, 480)       388736    \n",
      "_________________________________________________________________\n",
      "maxpool_3 (MaxPooling2D)     (None, 14, 14, 480)       0         \n",
      "_________________________________________________________________\n",
      "inception_4a (Inception)     (None, 14, 14, 512)       376176    \n",
      "_________________________________________________________________\n",
      "inception_4b (Inception)     (None, 14, 14, 512)       449160    \n",
      "_________________________________________________________________\n",
      "inception_4c (Inception)     (None, 14, 14, 512)       510104    \n",
      "_________________________________________________________________\n",
      "inception_4d (Inception)     (None, 14, 14, 528)       605376    \n",
      "_________________________________________________________________\n",
      "inception_4e (Inception)     (None, 14, 14, 832)       868352    \n",
      "_________________________________________________________________\n",
      "maxpool_4 (MaxPooling2D)     (None, 7, 7, 832)         0         \n",
      "_________________________________________________________________\n",
      "inception_5a (Inception)     (None, 7, 7, 832)         1043456   \n",
      "_________________________________________________________________\n",
      "inception_5b (Inception)     (None, 7, 7, 1024)        1444080   \n",
      "_________________________________________________________________\n",
      "avgpool_1 (AveragePooling2D) (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "output_flatten (Flatten)     (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output_dropout (Dropout)     (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "output_dense (Dense)         (None, 1000)              1025000   \n",
      "_________________________________________________________________\n",
      "aux_3 (Softmax)              (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 6,998,552\n",
      "Trainable params: 6,998,552\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def GoogLeNet(im_height=224, im_width=224, class_num=1000, aux_logits=False):\n",
    "    # tensorflow中的tensor通道排序是NHWC\n",
    "    input_image = layers.Input(shape=(im_height, im_width, 3), dtype=\"float32\")\n",
    "    # (None, 224, 224, 3)\n",
    "    x = layers.Conv2D(64, kernel_size=7, strides=2, padding=\"SAME\", activation=\"relu\", name=\"conv2d_1\")(input_image)\n",
    "    # (None, 112, 112, 64)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_1\")(x)\n",
    "    # (None, 56, 56, 64)\n",
    "    x = layers.Conv2D(64, kernel_size=1, activation=\"relu\", name=\"conv2d_2\")(x)\n",
    "    # (None, 56, 56, 64)\n",
    "    x = layers.Conv2D(192, kernel_size=3, padding=\"SAME\", activation=\"relu\", name=\"conv2d_3\")(x)\n",
    "    # (None, 56, 56, 192)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_2\")(x)\n",
    "\n",
    "    # (None, 28, 28, 192)\n",
    "    x = Inception(64, 96, 128, 16, 32, 32, name=\"inception_3a\")(x)\n",
    "    # (None, 28, 28, 256)\n",
    "    x = Inception(128, 128, 192, 32, 96, 64, name=\"inception_3b\")(x)\n",
    "\n",
    "    # (None, 28, 28, 480)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_3\")(x)\n",
    "    # (None, 14, 14, 480)\n",
    "    x = Inception(192, 96, 208, 16, 48, 64, name=\"inception_4a\")(x)\n",
    "    if aux_logits:\n",
    "        aux1 = InceptionAux(class_num, name=\"aux_1\")(x)\n",
    "\n",
    "    # (None, 14, 14, 512)\n",
    "    x = Inception(160, 112, 224, 24, 64, 64, name=\"inception_4b\")(x)\n",
    "    # (None, 14, 14, 512)\n",
    "    x = Inception(128, 128, 256, 24, 64, 64, name=\"inception_4c\")(x)\n",
    "    # (None, 14, 14, 512)\n",
    "    x = Inception(112, 144, 288, 32, 64, 64, name=\"inception_4d\")(x)\n",
    "    if aux_logits:\n",
    "        aux2 = InceptionAux(class_num, name=\"aux_2\")(x)\n",
    "\n",
    "    # (None, 14, 14, 528)\n",
    "    x = Inception(256, 160, 320, 32, 128, 128, name=\"inception_4e\")(x)\n",
    "    # (None, 14, 14, 532)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\", name=\"maxpool_4\")(x)\n",
    "\n",
    "    # (None, 7, 7, 832)\n",
    "    x = Inception(256, 160, 320, 32, 128, 128, name=\"inception_5a\")(x)\n",
    "    # (None, 7, 7, 832)\n",
    "    x = Inception(384, 192, 384, 48, 128, 128, name=\"inception_5b\")(x)\n",
    "    # (None, 7, 7, 1024)\n",
    "    x = layers.AvgPool2D(pool_size=7, strides=1, name=\"avgpool_1\")(x)\n",
    "\n",
    "    # (None, 1, 1, 1024)\n",
    "    x = layers.Flatten(name=\"output_flatten\")(x)\n",
    "    # (None, 1024)\n",
    "    x = layers.Dropout(rate=0.4, name=\"output_dropout\")(x)\n",
    "    x = layers.Dense(class_num, name=\"output_dense\")(x)\n",
    "    # (None, class_num)\n",
    "    aux3 = layers.Softmax(name=\"aux_3\")(x)\n",
    "\n",
    "    if aux_logits:\n",
    "        model = models.Model(inputs=input_image, outputs=[aux1, aux2, aux3])\n",
    "    else:\n",
    "        model = models.Model(inputs=input_image, outputs=aux3)\n",
    "    return model\n",
    "\n",
    "network=GoogLeNet()\n",
    "network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.InceptionAux at 0x23d71076a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class InceptionAux(layers.Layer):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(InceptionAux, self).__init__(**kwargs)\n",
    "        self.averagePool = layers.AvgPool2D(pool_size=5, strides=3)\n",
    "        self.conv = layers.Conv2D(128, kernel_size=1, activation=\"relu\")\n",
    "\n",
    "        self.fc1 = layers.Dense(1024, activation=\"relu\")\n",
    "        self.fc2 = layers.Dense(num_classes)\n",
    "        self.softmax = layers.Softmax()\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14\n",
    "        x = self.averagePool(inputs)\n",
    "        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4\n",
    "        x = self.conv(x)\n",
    "        # N x 128 x 4 x 4\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dropout(rate=0.5)(x)\n",
    "        # N x 2048\n",
    "        x = self.fc1(x)\n",
    "        x = layers.Dropout(rate=0.5)(x)\n",
    "        # N x 1024\n",
    "        x = self.fc2(x)\n",
    "        # N x num_classes\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "InceptionAux(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "cannot find D:\\jupyter\\data_set\\flower_data\\train",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-456584799cc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCategoricalAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test_accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-456584799cc1>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mvalidation_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"val\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cannot find {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cannot find {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: cannot find D:\\jupyter\\data_set\\flower_data\\train"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
    "    image_path = os.path.join(data_root, \"data_set\", \"flower_data\")  # flower data set path\n",
    "    train_dir = os.path.join(image_path, \"train\")\n",
    "    validation_dir = os.path.join(image_path, \"val\")\n",
    "    assert os.path.exists(train_dir), \"cannot find {}\".format(train_dir)\n",
    "    assert os.path.exists(validation_dir), \"cannot find {}\".format(validation_dir)\n",
    "\n",
    "    # create direction for saving weights\n",
    "    if not os.path.exists(\"save_weights\"):\n",
    "        os.makedirs(\"save_weights\")\n",
    "\n",
    "    im_height = 224\n",
    "    im_width = 224\n",
    "    batch_size = 32\n",
    "    epochs = 30\n",
    "    \n",
    "    def pre_function(img):\n",
    "        # img = im.open('test.jpg')\n",
    "        # img = np.array(img).astype(np.float32)\n",
    "        img = img / 255.\n",
    "        img = (img - 0.5) * 2.0\n",
    "\n",
    "        return img\n",
    "\n",
    "    # data generator with data augmentation\n",
    "    train_image_generator = ImageDataGenerator(preprocessing_function=pre_function,\n",
    "                                               horizontal_flip=True)\n",
    "    validation_image_generator = ImageDataGenerator(preprocessing_function=pre_function)\n",
    "\n",
    "    train_data_gen = train_image_generator.flow_from_directory(directory=train_dir,\n",
    "                                                               batch_size=batch_size,\n",
    "                                                               shuffle=True,\n",
    "                                                               target_size=(im_height, im_width),\n",
    "                                                               class_mode='categorical')\n",
    "    total_train = train_data_gen.n\n",
    "\n",
    "    # get class dict\n",
    "    class_indices = train_data_gen.class_indices\n",
    "\n",
    "    # transform value and key of dict\n",
    "    inverse_dict = dict((val, key) for key, val in class_indices.items())\n",
    "    # write dict into json file\n",
    "    json_str = json.dumps(inverse_dict, indent=4)\n",
    "    with open('class_indices.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    val_data_gen = validation_image_generator.flow_from_directory(directory=validation_dir,\n",
    "                                                                  batch_size=batch_size,\n",
    "                                                                  shuffle=False,\n",
    "                                                                  target_size=(im_height, im_width),\n",
    "                                                                  class_mode='categorical')\n",
    "    total_val = val_data_gen.n\n",
    "    print(\"using {} images for training, {} images for validation.\".format(total_train,\n",
    "                                                                           total_val))\n",
    "\n",
    "    model = GoogLeNet(im_height=im_height, im_width=im_width, class_num=5, aux_logits=True)\n",
    "    # model.build((batch_size, 224, 224, 3))  # when using subclass model\n",
    "    model.summary()\n",
    "\n",
    "    # using keras low level api for training\n",
    "    loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int32' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3ea96acec000>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mpre_deal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m47\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m89\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-3ea96acec000>\u001b[0m in \u001b[0;36mpre_deal\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#波形指标\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mxp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mda_waveform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mda_rms\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mx_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mone_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda_waveform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.int32' object is not iterable"
     ]
    }
   ],
   "source": [
    "def pre_deal(data):\n",
    "    pre_data=[]\n",
    "    for i in data:\n",
    "        one_data=[]\n",
    "        #均值\n",
    "        da_mean=i.mean()\n",
    "        one_data.append(da_mean)\n",
    "        #方差\n",
    "        da_var=i.var()\n",
    "        one_data.append(da_var)\n",
    "        #标准差\n",
    "        da_std=i.std()\n",
    "        one_data.append(da_std)\n",
    "        #均方根\n",
    "        da_msv=da_mean ** 2 + da_var\n",
    "        da_rms = da_msv ** 0.5\n",
    "        one_data.append(da_rms)\n",
    "        #波形指标\n",
    "        x_ = abs(i).mean()\n",
    "        xp = max(i)\n",
    "        da_waveform = da_rms / x_\n",
    "        one_data.append(da_waveform)\n",
    "        #峰值指标\n",
    "        da_peak = xp / da_rms\n",
    "        one_data.append(da_peak)\n",
    "        #脉冲指标\n",
    "        da_impluse = xp / x_\n",
    "        one_data.append(da_impluse)\n",
    "        #裕度指标\n",
    "        da_clearance = xp / (sum([math.sqrt(i) for i in abs(i)]) / len(i)) ** 2\n",
    "        one_data.append(da_clearance)\n",
    "        #偏斜度指标\n",
    "        da_alpha = pd.Series(i).skew()\n",
    "        da_skew = da_alpha / da_std ** 3\n",
    "        one_data.append(da_skew)\n",
    "        #峭度指标\n",
    "        da_beta = pd.Series(i).kurt()\n",
    "        da_kurt = da_beta / da_std ** 4\n",
    "        one_data.append(da_kurt)\n",
    "        \n",
    "        pre_data.append(one_data)\n",
    "    return np.array(pre_data)\n",
    "\n",
    "pre_deal(np.array([2,5,6,47,89,8,5,5,4,6,7,5,2,3,4,5,0,9,6,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-642e4e521cb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#n_components: 保留的主成分;whiten 归一化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# X_pca 可以代表原来的数据，但是经过矩阵运算，数据值已经变化（脱敏数据），属性也就没有实际的物理意义\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX_pca_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA #降维（）\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#PCA 主成分分析\n",
    "pca=PCA(n_components=0.9,whiten=True)#n_components: 保留的主成分;whiten 归一化\n",
    "# X_pca 可以代表原来的数据，但是经过矩阵运算，数据值已经变化（脱敏数据），属性也就没有实际的物理意义\n",
    "X_pca_train=pca.fit_transform(X)\n",
    "\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "clf2=SVC(C=1.0,kernel='rbf')\n",
    "clf2.fit(X_train,Y_train)\n",
    "clf2.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #数据获取，网页解析\n",
    "import re  #正则表达式，文字匹配\n",
    "import urllib.request,urllib.error  #制定url，获取网页数据\n",
    "import xlwt #进行excle操作\n",
    "import sqlite3 #进行数据库操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    baseurl='https://movie.douban.com/top250?start='\n",
    "    #1.爬取网页\n",
    "    datalist=getdata(baseurl)\n",
    "    \n",
    "    \n",
    "    #3.保存数据\n",
    "    savepath='./douban/top250.xls'\n",
    "    savedata(savepath)\n",
    "    return\n",
    "\n",
    "# 电影连接\n",
    "findLink=re.compile(r'<a href=\"(.*?)\">')\n",
    "# 图片连接\n",
    "findImgSrc=re.compile(r'<img.*src=\"(.*?)\"',re.S)\n",
    "# 电影名字\n",
    "findTitle=re.compile(r'<span class=\"title\">(.*)</span>')\n",
    "# 评分\n",
    "findRating=re.compile(r'<span class=\"rating_num\" property=\"v:average\">(.*)</span>')\n",
    "# 品论人数\n",
    "findnum=re.compile(r'<span>(\\d*)人评价</span>')\n",
    "# 评价\n",
    "findInq=re.compile(r'<span class=\"inq\">(.*)</span>')\n",
    "# 相关内容\n",
    "findDb=re.compile(r'<p class=\"\">(.*)</p>',re.S)\n",
    "\n",
    "#获取网页\n",
    "def getdata(baseurl):\n",
    "    datalist=[]\n",
    "    for i in range(0,10):\n",
    "\n",
    "        url=baseurl+str(i*25)\n",
    "        html=askurl(url)\n",
    "    \n",
    "        #2.逐一解析数据\n",
    "        soup=BeautifulSoup(html,'lxml')\n",
    "        for item in soup.find_all('div',class_='item'):\n",
    "            data=[]\n",
    " \n",
    "            item=str(item)\n",
    "            \n",
    "            # 获取影片的链接\n",
    "            link=re.findall(findLink,item)[0] #给一个查找字符串规则\n",
    "            data.append(link)\n",
    "            # 获取图片\n",
    "            imgsrc=re.findall(findImgSrc,item)[0]\n",
    "            data.append(imgsrc)\n",
    "            # 电影名字\n",
    "            name=re.findall(findTitle,item)\n",
    "            if (len(name)==2):\n",
    "                ctitle=name[0]\n",
    "                data.append(ctitle)\n",
    "                otitle=name[1].replace(\"/\",\"\")\n",
    "                data.append(otitle)\n",
    "            else:\n",
    "                data.append(name[0])\n",
    "                data.append(' ')#留空\n",
    "            # 评分\n",
    "            rate=re.findall(findRating,item)[0]\n",
    "            data.append(rate)\n",
    "            # 品论人数\n",
    "            people_num=re.findall(findnum,item)[0]\n",
    "            data.append(people_num)\n",
    "            # 评价\n",
    "            Inq=re.findall(findInq,item)\n",
    "            if len(Inq)!=0:\n",
    "                Inq=Inq[0].replace('。','')\n",
    "                data.append(Inq)\n",
    "            else:\n",
    "                data.append(' ')\n",
    "            # 相关内容\n",
    "            Db=re.findall(findDb,item)[0]\n",
    "            Db=re.sub('<br(\\s+)?/>(\\s+)?',' ',Db)#替换<br(\\s+)?/>(\\s+)?\n",
    "            Db=re.sub('/',' ',Db)\n",
    "            data.append(Db.strip())#.strip(),去掉Db内的空格\n",
    "            \n",
    "            \n",
    "            datalist.append(data)\n",
    "    \n",
    "    return datalist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askurl(url):\n",
    "    #模拟浏览器头部信息，向豆瓣服务器发送信息\n",
    "    header={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "        AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"\n",
    "    }# 用户代理，表示告诉豆瓣服务器，我们是什么类型的机型，浏览器\n",
    "    \n",
    "    request=urllib.request.Request(url,headers=header)\n",
    "    html=''\n",
    "    try:\n",
    "        response=urllib.request.urlopen(request)\n",
    "        html=response.read().decode('utf-8')\n",
    "\n",
    "    except urllib.error.URLError as e:\n",
    "        # 返回对象是否具有给定名称的属性\n",
    "        if hasattr(e,'code'):\n",
    "            print(e.code)\n",
    "        if hasattr(e,'reason'):\n",
    "            print(e.reason)\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "\n",
    "# 保存数据\n",
    "def savedata(savepath):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://movie.douban.com/subject/1292052/',\n",
       " 'https://img2.doubanio.com/view/photo/s_ratio_poster/public/p480747492.jpg',\n",
       " '肖申克的救赎',\n",
       " '\\xa0\\xa0The Shawshank Redemption',\n",
       " '9.7',\n",
       " '2231662',\n",
       " '希望让人自由',\n",
       " '导演: 弗兰克·德拉邦特 Frank Darabont\\xa0\\xa0\\xa0主演: 蒂姆·罗宾斯 Tim Robbins  ... 1994\\xa0 \\xa0美国\\xa0 \\xa0犯罪 剧情\\n                        < p>\\n<div class=\"star\">\\n<span class=\"rating5-t\">< span>\\n<span class=\"rating_num\" property=\"v:average\">9.7< span>\\n<span content=\"10.0\" property=\"v:best\">< span>\\n<span>2231662人评价< span>\\n< div>\\n<p class=\"quote\">\\n<span class=\"inq\">希望让人自由。< span>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalist=getdata('https://movie.douban.com/top250?start=')\n",
    "datalist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
