{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scihub 下载外文文献"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Sci-Hub:Failed to fetch pdf with identifier { (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier { (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"shell_port\": 58821, (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"shell_port\": 58821, (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"iopub_port\": 58822, (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"iopub_port\": 58822, (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"stdin_port\": 58823, (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"stdin_port\": 58823, (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"control_port\": 58825, (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"control_port\": 58825, (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"hb_port\": 58824, (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"hb_port\": 58824, (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"ip\": \"127.0.0.1\", (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"ip\": \"127.0.0.1\", (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"key\": \"3e912453-b6767f6b2dd566652a8f2c91\", (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"key\": \"3e912453-b6767f6b2dd566652a8f2c91\", (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"transport\": \"tcp\", (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"transport\": \"tcp\", (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"signature_scheme\": \"hmac-sha256\", (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"signature_scheme\": \"hmac-sha256\", (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier   \"kernel_name\": \"\" (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier   \"kernel_name\": \"\" (resolved url None) due to request exception.\n",
      "INFO:Sci-Hub:Failed to fetch pdf with identifier } (resolved url None) due to request exception.\n",
      "DEBUG:Sci-Hub:Failed to fetch pdf with identifier } (resolved url None) due to request exception.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import argparse\n",
    "import hashlib\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from retrying import retry\n",
    "\n",
    "# log config\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger('Sci-Hub')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "# constants\n",
    "SCHOLARS_BASE_URL = 'https://www.sciencedirect.com/search'\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:27.0) Gecko/20100101 Firefox/27.0'}\n",
    "\n",
    "class SciHub(object):\n",
    "    \"\"\"\n",
    "    SciHub class can search for papers on Google Scholars \n",
    "    and fetch/download papers from sci-hub.io\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.sess = requests.Session()\n",
    "        self.sess.headers = HEADERS\n",
    "        self.available_base_url_list = self._get_available_scihub_urls()\n",
    "        self.base_url = self.available_base_url_list[0] + '/'\n",
    "\n",
    "    def _get_available_scihub_urls(self):\n",
    "        '''\n",
    "        Finds available scihub urls via http://tool.yovisun.com/scihub/\n",
    "        '''\n",
    "        urls = []\n",
    "        res = requests.get('http://tool.yovisun.com/scihub/')\n",
    "        s = self._get_soup(res.content)\n",
    "        for a in s.find_all('a', href=True):\n",
    "            if 'sci-hub.' in a['href']:\n",
    "                urls.append(a['href'])\n",
    "        return urls\n",
    "\n",
    "    def set_proxy(self, proxy):\n",
    "        '''\n",
    "        set proxy for session\n",
    "        :param proxy_dict:\n",
    "        :return:\n",
    "        '''\n",
    "        if proxy:\n",
    "            self.sess.proxies = {\n",
    "                \"http\": proxy,\n",
    "                \"https\": proxy, }\n",
    "\n",
    "    def _change_base_url(self):\n",
    "        if not self.available_base_url_list:\n",
    "            raise Exception('Ran out of valid sci-hub urls')\n",
    "        del self.available_base_url_list[0]\n",
    "        self.base_url = self.available_base_url_list[0] + '/'\n",
    "        logger.info(\"I'm changing to {}\".format(self.available_base_url_list[0]))\n",
    "\n",
    "    def search(self, query, limit=10, download=False):\n",
    "        \"\"\"\n",
    "        Performs a query on scholar.google.com, and returns a dictionary\n",
    "        of results in the form {'papers': ...}. Unfortunately, as of now,\n",
    "        captchas can potentially prevent searches after a certain limit.\n",
    "        \"\"\"\n",
    "        start = 0\n",
    "        results = {'papers': []}\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                res = self.sess.get(SCHOLARS_BASE_URL, params={'qs': query, 'start': start})\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                results['err'] = 'Failed to complete search with query %s (connection error)' % query\n",
    "                return results\n",
    "\n",
    "            s = self._get_soup(res.content)\n",
    "            papers = s.find_all('div', class_=\"result-item-content\")\n",
    "\n",
    "            if not papers:\n",
    "                if 'CAPTCHA' in str(res.content):\n",
    "                    results['err'] = 'Failed to complete search with query %s (captcha)' % query\n",
    "                return results\n",
    "\n",
    "            for paper in papers:\n",
    "                if not paper.find('table'):\n",
    "                    source = None\n",
    "                    # 统一使用scihub下载，因此去除pdf链接爬取\n",
    "                    # pdf = paper.find('span', class_='preview-link')\n",
    "                    # source = pdf.find('a')['href']\n",
    "                    link = paper.find('h2')\n",
    "\n",
    "                    if link.find('a'):\n",
    "                        source = link.find('a')['href']\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    results['papers'].append({\n",
    "                        'name': link.text,\n",
    "                        'url': f\"https://www.sciencedirect.com{source}\"\n",
    "                    })\n",
    "\n",
    "                    if len(results['papers']) >= limit:\n",
    "                        return results\n",
    "\n",
    "            start += 10\n",
    "\n",
    "    @retry(wait_random_min=100, wait_random_max=1000, stop_max_attempt_number=10)\n",
    "    def download(self, identifier, destination='', path=None):\n",
    "        \"\"\"\n",
    "        Downloads a paper from sci-hub given an indentifier (DOI, PMID, URL).\n",
    "        Currently, this can potentially be blocked by a captcha if a certain\n",
    "        limit has been reached.\n",
    "        \"\"\"\n",
    "        data = self.fetch(identifier)\n",
    "\n",
    "        if not 'err' in data:\n",
    "            self._save(data['pdf'],\n",
    "                       os.path.join(destination, path if path else data['name']))\n",
    "\n",
    "        return data\n",
    "\n",
    "    def fetch(self, identifier):\n",
    "        \"\"\"\n",
    "        Fetches the paper by first retrieving the direct link to the pdf.\n",
    "        If the indentifier is a DOI, PMID, or URL pay-wall, then use Sci-Hub\n",
    "        to access and download paper. Otherwise, just download paper directly.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            url = self._get_direct_url(identifier)\n",
    "\n",
    "            # verify=False is dangerous but sci-hub.io \n",
    "            # requires intermediate certificates to verify\n",
    "            # and requests doesn't know how to download them.\n",
    "            # as a hacky fix, you can add them to your store\n",
    "            # and verifying would work. will fix this later.\n",
    "            res = self.sess.get(url, verify=False)\n",
    "\n",
    "            if res.headers['Content-Type'] != 'application/pdf':\n",
    "                self._change_base_url()\n",
    "                logger.info('Failed to fetch pdf with identifier %s '\n",
    "                                           '(resolved url %s) due to captcha' % (identifier, url))\n",
    "                raise CaptchaNeedException('Failed to fetch pdf with identifier %s '\n",
    "                                           '(resolved url %s) due to captcha' % (identifier, url))\n",
    "                # return {\n",
    "                #     'err': 'Failed to fetch pdf with identifier %s (resolved url %s) due to captcha'\n",
    "                #            % (identifier, url)\n",
    "                # }\n",
    "            else:\n",
    "                return {\n",
    "                    'pdf': res.content,\n",
    "                    'url': url,\n",
    "                    'name': self._generate_name(res)\n",
    "                }\n",
    "\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            logger.info('Cannot access {}, changing url'.format(self.available_base_url_list[0]))\n",
    "            self._change_base_url()\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.info('Failed to fetch pdf with identifier %s (resolved url %s) due to request exception.'\n",
    "                       % (identifier, url))\n",
    "            return {\n",
    "                'err': 'Failed to fetch pdf with identifier %s (resolved url %s) due to request exception.'\n",
    "                       % (identifier, url)\n",
    "            }\n",
    "\n",
    "    def _get_direct_url(self, identifier):\n",
    "        \"\"\"\n",
    "        Finds the direct source url for a given identifier.\n",
    "        \"\"\"\n",
    "        id_type = self._classify(identifier)\n",
    "\n",
    "        return identifier if id_type == 'url-direct' \\\n",
    "            else self._search_direct_url(identifier)\n",
    "\n",
    "    def _search_direct_url(self, identifier):\n",
    "        \"\"\"\n",
    "        Sci-Hub embeds papers in an iframe. This function finds the actual\n",
    "        source url which looks something like https://moscow.sci-hub.io/.../....pdf.\n",
    "        \"\"\"\n",
    "        res = self.sess.get(self.base_url + identifier, verify=False)\n",
    "        s = self._get_soup(res.content)\n",
    "        iframe = s.find('iframe')\n",
    "        if iframe:\n",
    "            return iframe.get('src') if not iframe.get('src').startswith('//') \\\n",
    "                else 'http:' + iframe.get('src')\n",
    "\n",
    "    def _classify(self, identifier):\n",
    "        \"\"\"\n",
    "        Classify the type of identifier:\n",
    "        url-direct - openly accessible paper\n",
    "        url-non-direct - pay-walled paper\n",
    "        pmid - PubMed ID\n",
    "        doi - digital object identifier\n",
    "        \"\"\"\n",
    "        if (identifier.startswith('http') or identifier.startswith('https')):\n",
    "            if identifier.endswith('pdf'):\n",
    "                return 'url-direct'\n",
    "            else:\n",
    "                return 'url-non-direct'\n",
    "        elif identifier.isdigit():\n",
    "            return 'pmid'\n",
    "        else:\n",
    "            return 'doi'\n",
    "\n",
    "    def _save(self, data, path):\n",
    "        \"\"\"\n",
    "        Save a file give data and a path.\n",
    "        \"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            f.write(data)\n",
    "\n",
    "    def _get_soup(self, html):\n",
    "        \"\"\"\n",
    "        Return html soup.\n",
    "        \"\"\"\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    def _generate_name(self, res):\n",
    "        \"\"\"\n",
    "        Generate unique filename for paper. Returns a name by calcuating \n",
    "        md5 hash of file contents, then appending the last 20 characters\n",
    "        of the url which typically provides a good paper identifier.\n",
    "        \"\"\"\n",
    "        name = res.url.split('/')[-1]\n",
    "        name = re.sub('#view=(.+)', '', name)\n",
    "        pdf_hash = hashlib.md5(res.content).hexdigest()\n",
    "        return '%s-%s' % (pdf_hash, name[-20:])\n",
    "\n",
    "class CaptchaNeedException(Exception):\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    sh = SciHub()\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='SciHub - To remove all barriers in the way of science.')\n",
    "    parser.add_argument('-d', '--download', metavar='(DOI|PMID|URL)', help='tries to find and download the paper',\n",
    "                        type=str)\n",
    "    parser.add_argument('-f', '--file', metavar='path', help='pass file with list of identifiers and download each',\n",
    "                        type=str)\n",
    "    parser.add_argument('-s', '--search', metavar='query', help='search Google Scholars', type=str)\n",
    "    parser.add_argument('-sd', '--search_download', metavar='query',\n",
    "                        help='search Google Scholars and download if possible', type=str)\n",
    "    parser.add_argument('-l', '--limit', metavar='N', help='the number of search results to limit to', default=10,\n",
    "                        type=int)\n",
    "    parser.add_argument('-o', '--output', metavar='path', help='directory to store papers', default='', type=str)\n",
    "    parser.add_argument('-v', '--verbose', help='increase output verbosity', action='store_true')\n",
    "    parser.add_argument('-p', '--proxy', help='via proxy format like socks5://user:pass@host:port', action='store', type=str)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.verbose:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    if args.proxy:\n",
    "        sh.set_proxy(args.proxy)\n",
    "\n",
    "    if args.download:\n",
    "        result = sh.download(args.download, args.output)\n",
    "        if 'err' in result:\n",
    "            logger.debug('%s', result['err'])\n",
    "        else:\n",
    "            logger.debug('Successfully downloaded file with identifier %s', args.download)\n",
    "    elif args.search:\n",
    "        results = sh.search(args.search, args.limit)\n",
    "        if 'err' in results:\n",
    "            logger.debug('%s', results['err'])\n",
    "        else:\n",
    "            logger.debug('Successfully completed search with query %s', args.search)\n",
    "        print(results)\n",
    "    elif args.search_download:\n",
    "        results = sh.search(args.search_download, args.limit)\n",
    "        if 'err' in results:\n",
    "            logger.debug('%s', results['err'])\n",
    "        else:\n",
    "            logger.debug('Successfully completed search with query %s', args.search_download)\n",
    "            for paper in results['papers']:\n",
    "                result = sh.download(paper['url'], args.output)\n",
    "                if 'err' in result:\n",
    "                    logger.debug('%s', result['err'])\n",
    "                else:\n",
    "                    logger.debug('Successfully downloaded file with identifier %s', paper['url'])\n",
    "    elif args.file:\n",
    "        with open(args.file, 'r') as f:\n",
    "            identifiers = f.read().splitlines()\n",
    "            for identifier in identifiers:\n",
    "                result = sh.download(identifier, args.output)\n",
    "                if 'err' in result:\n",
    "                    logger.debug('%s', result['err'])\n",
    "                else:\n",
    "                    logger.debug('Successfully downloaded file with identifier %s', identifier)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'papers': [{'name': 'Review of deep learning for photoacoustic imaging', 'url': 'https://www.sciencedirect.com/science/article/pii/S2213597920300550'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Sci-Hub:Failed to fetch pdf with identifier https://www.sciencedirect.com/science/article/pii/S2213597920300550 (resolved url None) due to request exception.\n"
     ]
    }
   ],
   "source": [
    "from scihub import SciHub\n",
    "\n",
    "sh = SciHub()\n",
    "\n",
    "# 搜索词\n",
    "keywords = \"deep learning\"\n",
    "\n",
    "# 搜索该关键词相关的论文，limit为篇数\n",
    "result = sh.search(keywords, limit=1)\n",
    "\n",
    "print(result)\n",
    "\n",
    "for index, paper in enumerate(result.get(\"papers\", [])):\n",
    "    # 批量下载这些论文\n",
    "    sh.download(paper[\"url\"], path=f\"files/{keywords.replace(' ', '_')}_{index}.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f40375b3417f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# from multiprocessing import Pool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# =============================================================================\n",
    "#  @article{zhang2017beyond,\n",
    "#    title={Beyond a {Gaussian} denoiser: Residual learning of deep {CNN} for image denoising},\n",
    "#    author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},\n",
    "#    journal={IEEE Transactions on Image Processing},\n",
    "#    year={2017},\n",
    "#    volume={26}, \n",
    "#    number={7}, \n",
    "#    pages={3142-3155}, \n",
    "#  }\n",
    "# by Kai Zhang (08/2018)\n",
    "# cskaizhang@gmail.com\n",
    "# https://github.com/cszn\n",
    "# modified on the code from https://github.com/SaoYan/DnCNN-PyTorch\n",
    "# =============================================================================\n",
    "\n",
    "# no need to run this code separately\n",
    "\n",
    "\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "# from multiprocessing import Pool\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "patch_size, stride = 40, 10\n",
    "aug_times = 1\n",
    "scales = [1, 0.9, 0.8, 0.7]\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "class DenoisingDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping tensors.\n",
    "    Arguments:\n",
    "        xs (Tensor): clean image patches\n",
    "        sigma: noise level, e.g., 25\n",
    "    \"\"\"\n",
    "    def __init__(self, xs, sigma):\n",
    "        super(DenoisingDataset, self).__init__()\n",
    "        self.xs = xs\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_x = self.xs[index]\n",
    "        noise = torch.randn(batch_x.size()).mul_(self.sigma/255.0)\n",
    "        batch_y = batch_x + noise\n",
    "        return batch_y, batch_x\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.xs.size(0)\n",
    "\n",
    "\n",
    "def show(x, title=None, cbar=False, figsize=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(x, interpolation='nearest', cmap='gray')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    if cbar:\n",
    "        plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def data_aug(img, mode=0):\n",
    "    # data augmentation\n",
    "    if mode == 0:\n",
    "        return img\n",
    "    elif mode == 1:\n",
    "        return np.flipud(img)\n",
    "    elif mode == 2:\n",
    "        return np.rot90(img)\n",
    "    elif mode == 3:\n",
    "        return np.flipud(np.rot90(img))\n",
    "    elif mode == 4:\n",
    "        return np.rot90(img, k=2)\n",
    "    elif mode == 5:\n",
    "        return np.flipud(np.rot90(img, k=2))\n",
    "    elif mode == 6:\n",
    "        return np.rot90(img, k=3)\n",
    "    elif mode == 7:\n",
    "        return np.flipud(np.rot90(img, k=3))\n",
    "\n",
    "\n",
    "def gen_patches(file_name):\n",
    "    # get multiscale patches from a single image\n",
    "    img = cv2.imread(file_name, 0)  # gray scale\n",
    "    h, w = img.shape\n",
    "    patches = []\n",
    "    for s in scales:\n",
    "        h_scaled, w_scaled = int(h*s), int(w*s)\n",
    "        img_scaled = cv2.resize(img, (h_scaled, w_scaled), interpolation=cv2.INTER_CUBIC)\n",
    "        # extract patches\n",
    "        for i in range(0, h_scaled-patch_size+1, stride):\n",
    "            for j in range(0, w_scaled-patch_size+1, stride):\n",
    "                x = img_scaled[i:i+patch_size, j:j+patch_size]\n",
    "                for k in range(0, aug_times):\n",
    "                    x_aug = data_aug(x, mode=np.random.randint(0, 8))\n",
    "                    patches.append(x_aug)\n",
    "    return patches\n",
    "\n",
    "\n",
    "def datagenerator(data_dir='data/Train400', verbose=False):\n",
    "    # generate clean patches from a dataset\n",
    "    file_list = glob.glob(data_dir+'/*.png')  # get name list of all .png files\n",
    "    # initrialize\n",
    "    data = []\n",
    "    # generate patches\n",
    "    for i in range(len(file_list)):\n",
    "        patches = gen_patches(file_list[i])\n",
    "        for patch in patches:    \n",
    "            data.append(patch)\n",
    "        if verbose:\n",
    "            print(str(i+1) + '/' + str(len(file_list)) + ' is done ^_^')\n",
    "    data = np.array(data, dtype='uint8')\n",
    "    data = np.expand_dims(data, axis=3)\n",
    "    discard_n = len(data)-len(data)//batch_size*batch_size  # because of batch namalization\n",
    "    data = np.delete(data, range(discard_n), axis=0)\n",
    "    print('^_^-training data finished-^_^')\n",
    "    return data\n",
    "\n",
    "\n",
    "# if __name__ == '__main__': \n",
    "\n",
    "#     data = datagenerator(data_dir='data/Train400')\n",
    "\n",
    "\n",
    "#    print('Shape of result = ' + str(res.shape))\n",
    "#    print('Saving data...')\n",
    "#    if not os.path.exists(save_dir):\n",
    "#            os.mkdir(save_dir)\n",
    "#    np.save(save_dir+'clean_patches.npy', res)\n",
    "#    print('Done.')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
