{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9add5ff7c7a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;34m\"User-Agent\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     }\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mhotSearch_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//tbody/tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "#爬取微博热搜话题，并进行数据清洗和本地化存储\n",
    "import requests\n",
    "import pickle\n",
    "from lxml import etree\n",
    "if __name__ == \"__main__\":\n",
    "    trending_url = \"https://s.weibo.com/top/summary\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"\n",
    "    }\n",
    "    page = requests.get(url, headers).text\n",
    "    tree = etree.HTML(page)\n",
    "    hotSearch_list = tree.xpath('//tbody/tr')\n",
    "    del hotSearch_list[0]\n",
    "    hotSearch_all_data = []\n",
    "    for hotSearch in hotSearch_list:\n",
    "        rank = hotSearch.xpath('./td[@class=\"td-01 ranktop\"]/text()')[0]\n",
    "        title = hotSearch.xpath('./td[@class=\"td-02\"]/a/text()')[0]\n",
    "        hot_degree = hotSearch.xpath('./td[@class=\"td-02\"]/span/text()')[0]\n",
    "        hotSearch_data = [rank, title, hot_degree]\n",
    "        hotSearch_all_data.append(hotSearch_data)\n",
    "    with open('./微博热搜话题.pkl', 'wb') as f:\n",
    "        pickle.dump(hotSearch_all_data, f)\n",
    "    print(\"数据保存成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-70aff6d5b455>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-70aff6d5b455>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    tren\tding_file.close()\u001b[0m\n\u001b[1;37m        \t^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#加载本地数据进行可视化展示\n",
    "import pickle\n",
    "import pandas as pd\n",
    "trending_file = open('./微博热搜话题.pkl', 'rb')\n",
    "data = pickle.load(trending_file)\n",
    "tren\tding_file.close()\n",
    "nu = []\n",
    "for i in range(50):\n",
    "    nu.append('')\n",
    "frame = pd.DataFrame(data, index=nu, columns=[\"排名\", \"关键词\", \"热度\"])\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    vocabulary_file.close()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# 取出数据中的标题进行关键词提取\n",
    "import pickle\n",
    "from ltp import LTP\n",
    "\n",
    "trending_file = open('./微博热搜话题.pkl', 'rb')\n",
    "data = pickle.load(trending_file)\n",
    "tren\tding_file.close()\n",
    "title = []\n",
    "for i in data:\n",
    "    title.append(i[1])\n",
    "ltp = LTP()\n",
    "piece_word = []\n",
    "piece_word_type=[]\n",
    "for t in title:\n",
    "    seg, hidden = ltp.seg([t])\n",
    "    ner = ltp.ner(hidden)\n",
    "    if len(ner[0])==0:\n",
    "        continue\n",
    "    for ner_t in ner[0]:\n",
    "        tag, start, end = ner_t\n",
    "        piece_word.append(\"\".join(seg[0][start:end + 1]))\n",
    "        piece_word_type.append(tag)\n",
    "    with open('./分词结果1.pkl', 'wb') as vocabulary_file:\n",
    "        pickle.dump(piece_word, vocabulary_file)\n",
    "     vocabulary_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-433c746bb01a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvocabulary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./分词结果1.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# 对分词结果进行展示\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "vocabulary_file = open('./分词结果1.pkl', 'rb')\n",
    "data = pickle.load(vocabulary_file)\n",
    "vocabulary_file.close()\n",
    "text_string=''\n",
    "for i in data:\n",
    "    text_string = text_string+ i +' '\n",
    "font = \"./PingFang.ttc\"\n",
    "wordcloud = WordCloud(font_path=font,width=600,height=600,background_color='white',).generate(text_string)\n",
    "# wordcloud.to_file('./images/2020.png')\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "myfontdict = {'fontsize':80}\n",
    "plt.title('2021年1月6日热搜',fontdict = myfontdict)\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ltp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5405ded23706>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlxml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mltp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLTP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ltp'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "from ltp import LTP\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.cnblogs.com/sonder/p/13496173.html\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36\"\n",
    "    }\n",
    "    page = requests.get(url, headers).text\n",
    "    tree = etree.HTML(page)\n",
    "    hotSearch_all_data = tree.xpath('//div[@id=\"cnblogs_post_body\"]/p[5]//text()')\n",
    "\n",
    "    ltp = LTP()\n",
    "    piece_word = []\n",
    "    for t in hotSearch_all_data:\n",
    "        seg, hidden = ltp.seg([t])\n",
    "        ner = ltp.ner(hidden)\n",
    "        if len(ner[0])==0:\n",
    "            continue\n",
    "        for ner_t in ner[0]:\n",
    "            tag, start, end = ner_t\n",
    "            piece_word.append(\"\".join(seg[0][start:end + 1]))\n",
    "    text_string=''\n",
    "    for i in piece_word:\n",
    "        text_string = text_string+ i +' '\n",
    "    font = \"./PingFang.ttc\"\n",
    "    wordcloud = WordCloud(font_path=font,width=600,height=600,background_color='white',).generate(text_string)\n",
    "    # wordcloud.to_file('./images/2020.png')\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    myfontdict = {'fontsize':80}\n",
    "    plt.title('2020年热搜集合',fontdict = myfontdict)\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
