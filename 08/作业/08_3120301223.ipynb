{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.爬取文章\n",
    "### coding = UTF-8\n",
    "### 网址：http://www.math.pku.edu.cn/teachers/lidf/docs/textrick/index.htm\n",
    " \n",
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    " \n",
    "### open the url and read\n",
    "\n",
    "    def getHtml(url):\n",
    "        page = urllib.request.urlopen(url)\n",
    "        html = page.read()\n",
    "        page.close()\n",
    "        return html\n",
    " \n",
    "### compile the regular expressions and find\n",
    "### all stuff we need\n",
    "\n",
    "    def getUrl(html):\n",
    "        reg = r'(?:href|HREF)=\"?((?:http://)?.+?\\.pdf)'\n",
    "        url_re = re.compile(reg)\n",
    "        url_lst = url_re.findall(html.decode('gb2312'))\n",
    "        return(url_lst)\n",
    "\n",
    "    def getFile(url):\n",
    "        file_name = url.split('/')[-1]\n",
    "        u = urllib.request.urlopen(url)\n",
    "        f = open(file_name, 'wb')\n",
    " \n",
    "    block_sz = 8192\n",
    "    while True:\n",
    "        buffer = u.read(block_sz)\n",
    "        if not buffer:\n",
    "            break\n",
    " \n",
    "        f.write(buffer)\n",
    "    f.close()\n",
    "    print (\"Sucessful to download\" + \" \" + file_name)\n",
    " \n",
    " \n",
    "    root_url = 'http://www.math.pku.edu.cn/teachers/lidf/docs/textrick/'\n",
    "\n",
    "    raw_url = 'http://www.math.pku.edu.cn/teachers/lidf/docs/textrick/index.htm'\n",
    "\n",
    "    html = getHtml(raw_url)\n",
    "    url_lst = getUrl(html)\n",
    "\n",
    "    os.mkdir('pdf_download')\n",
    "    os.chdir(os.path.join(os.getcwd(), 'pdf_download'))\n",
    "\n",
    "    for url in url_lst[:]:\n",
    "        url = root_url + url\n",
    "        getFile(url)\n",
    "    \n",
    "    \n",
    "## 2.将PDF转换为TXT格式\n",
    "\n",
    "    def readPDF(path, toPath):\n",
    "### 以二进制形式打开pdf文件\n",
    "    with open(path, \"rb\") as f:\n",
    "### 创建一个pdf文档分析器\n",
    "        parser = PDFParser(f)\n",
    "### 创建pdf文档\n",
    "        pdfFile = PDFDocument()\n",
    "### 链接分析器与文档对象\n",
    "        parser.set_document(pdfFile)\n",
    "        pdfFile.set_parser(parser)\n",
    " ### 提供初始化密码\n",
    "        pdfFile.initialize()\n",
    "### 检测文档是否提供txt转换\n",
    "    if not pdfFile.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "    else:\n",
    "### 解析数据\n",
    "### 数据管理\n",
    "        manager = PDFResourceManager()\n",
    "### 创建一个PDF设备对象\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(manager, laparams=laparams)\n",
    "### 解释器对象\n",
    "        interpreter = PDFPageInterpreter(manager, device)\n",
    "\n",
    "### 开始循环处理，每次处理一页\n",
    "        for page in pdfFile.get_pages():\n",
    "            interpreter.process_page(page)\n",
    "            layout = device.get_result()\n",
    "            for x in layout:\n",
    "                if(isinstance(x, LTTextBoxHorizontal)):\n",
    "                    with open(toPath, \"a\",encoding='utf-8') as f:\n",
    "                        str = x.get_text()\n",
    "                        # print(str)\n",
    "                        f.write(str+\"\\n\")\n",
    "## 3.提取TXT的关键词\n",
    "\n",
    "    def getKeywords(filename,filename_key):\n",
    "        file = open(filename, 'r',encoding='utf-8')\n",
    "        file_k = open(filename_key,'a',encoding='utf-8')\n",
    "        content = file.read()\n",
    "        startStr = 'Keywords:'\n",
    "        endStr = '\\n\\n'\n",
    "        try:\n",
    "            startIndex = content.index(startStr)\n",
    "            endIndex = content[startIndex:].index(endStr)\n",
    "            con_key = content[startIndex+len(startStr):startIndex+endIndex]\n",
    "            file.close()\n",
    "            file_k.write(con_key)\n",
    "            file_k.write('\\n')\n",
    "            file_k.close()\n",
    "        except:\n",
    "           print('该文件无关键字')\n",
    "            file.close()\n",
    "            file_k.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 由于爬虫结果没有实现，所以又做了一个爬取天气温度的例子\n",
    "\n",
    "## 1.爬取天气数据\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    import html5lib\n",
    "    import csv\n",
    "    def parse_page(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36',\n",
    "    }\n",
    "    response = requests.get(url)\n",
    "    text = response.content.decode('utf-8')\n",
    "    # 需要用到html5lib解析器，去补全html标签\n",
    "    soup = BeautifulSoup(text,'html5lib')\n",
    "    conMidtab = soup.find('div',class_='conMidtab')\n",
    "    tables = conMidtab.find_all('table')\n",
    "    for table in tables:\n",
    "        trs = table.find_all('tr')[2:]\n",
    "        for index,tr in enumerate(trs):\n",
    "            tds = tr.find_all('td')\n",
    "            city_td = tds[0]\n",
    "            if index == 0:\n",
    "                city_td = tds[1]\n",
    "            city = list(city_td.stripped_strings)[0]\n",
    "            temp_td = tds[-2]\n",
    "            temp = list(temp_td.stripped_strings)[0]\n",
    "## 2.将数据存储到csv文件\n",
    "\n",
    "            header = ['city', 'temp'] #数据列名\n",
    "            f = open('文件名.csv','w',encoding='utf-8')\n",
    "            csv_writer = csv.writer(f)\n",
    "            list2 = [{'city':city,'temp':temp}]\n",
    "            csv_writer.writerow([city,temp])\n",
    "            csv_writer.writerow([city,temp])\n",
    "            \n",
    "    def main():\n",
    "        url_list = [\n",
    "            'http://www.weather.com.cn/textFC/hb.shtml',\n",
    "            'http://www.weather.com.cn/textFC/db.shtml',\n",
    "            'http://www.weather.com.cn/textFC/hd.shtml',\n",
    "            'http://www.weather.com.cn/textFC/hz.shtml',\n",
    "            'http://www.weather.com.cn/textFC/hn.shtml',\n",
    "            'http://www.weather.com.cn/textFC/xb.shtml',\n",
    "            'http://www.weather.com.cn/textFC/xn.shtml',\n",
    "            'http://www.weather.com.cn/textFC/gat.shtml',\n",
    "        ]\n",
    "        for url in url_list:\n",
    "            parse_page(url)\n",
    "          if __name__ == '__main__':\n",
    "            main()  \n",
    "## 3.数据可视化\n",
    "\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    df = pd.read_csv('文件名.csv')\n",
    "    x = df.values[:,0] #city\n",
    "    y = df.values[:,1] #temp\n",
    "    plt.plot(x, y, 'o-',label=u\"线条\")\n",
    "    plt.title(u\"全国各地温度折线图\")\n",
    "    plt.xlabel(u\"city\")\n",
    "    plt.ylabel(u\"temp\")\n",
    "    plt.show()\n",
    "    plt.savefig(\"temp.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
