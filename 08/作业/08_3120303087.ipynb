{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyocr\n",
    "import importlib\n",
    "import sys\n",
    "import time\n",
    " \n",
    "importlib.reload(sys)\n",
    "time1 = time.time()\n",
    "# print(\"初始时间为：\",time1)\n",
    " \n",
    "import os.path\n",
    "from pdfminer.pdfparser import  PDFParser,PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import PDFPageAggregator\n",
    "from pdfminer.layout import LTTextBoxHorizontal,LAParams\n",
    "from pdfminer.pdfinterp import PDFTextExtractionNotAllowed\n",
    " \n",
    "text_path = r'words-words.pdf'\n",
    "# text_path = r'photo-words.pdf'\n",
    " \n",
    "def parse():\n",
    "    '''解析PDF文本，并保存到TXT文件中'''\n",
    "    fp = open(text_path,'rb')\n",
    "    #用文件对象创建一个PDF文档分析器\n",
    "    parser = PDFParser(fp)\n",
    "    #创建一个PDF文档\n",
    "    doc = PDFDocument()\n",
    "    #连接分析器，与文档对象\n",
    "    parser.set_document(doc)\n",
    "    doc.set_parser(parser)\n",
    " \n",
    "    #提供初始化密码，如果没有密码，就创建一个空的字符串\n",
    "    doc.initialize()\n",
    " \n",
    "    #检测文档是否提供txt转换，不提供就忽略\n",
    "    if not doc.is_extractable:\n",
    "        raise PDFTextExtractionNotAllowed\n",
    "    else:\n",
    "        #创建PDF，资源管理器，来共享资源\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        #创建一个PDF设备对象\n",
    "        laparams = LAParams()\n",
    "        device = PDFPageAggregator(rsrcmgr,laparams=laparams)\n",
    "        #创建一个PDF解释其对象\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr,device)\n",
    " \n",
    "        #循环遍历列表，每次处理一个page内容\n",
    "        # doc.get_pages() 获取page列表\n",
    "        for page in doc.get_pages():\n",
    "            interpreter.process_page(page)\n",
    "            #接受该页面的LTPage对象\n",
    "            layout = device.get_result()\n",
    "            # 这里layout是一个LTPage对象 里面存放着 这个page解析出的各种对象\n",
    "            # 一般包括LTTextBox, LTFigure, LTImage, LTTextBoxHorizontal 等等\n",
    "            # 想要获取文本就获得对象的text属性，\n",
    "            for x in layout:\n",
    "                if(isinstance(x,LTTextBoxHorizontal)):\n",
    "                    with open(r'blog.txt','a') as f:\n",
    "                        results = x.get_text()\n",
    "                        print(results)\n",
    "                        f.write(results  +\"\\n\")\n",
    "parse()\n",
    "\n",
    "#提取关键词\n",
    "def getKeywords(filename,filename_key):\n",
    "    file = open(filename, 'r',encoding='utf-8')\n",
    "    file_k = open(filename_key,'a',encoding='utf-8')\n",
    "    content = file.read()\n",
    "    startStr = 'Keywords:'\n",
    "    endStr = '\\n\\n'\n",
    "    try:\n",
    "        startIndex = content.index(startStr)\n",
    "        endIndex = content[startIndex:].index(endStr)\n",
    "        con_key = content[startIndex+len(startStr):startIndex+endIndex]\n",
    "        file.close()\n",
    "        file_k.write(con_key)\n",
    "        file_k.write('\\n')\n",
    "        file_k.close()\n",
    "    except:\n",
    "        print('该文件无关键字')\n",
    "        file.close()\n",
    "        file_k.close()\n",
    "                        \n",
    "#提取网页内容\n",
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "# open the url and read\n",
    "def getHtml(url):\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.102 Safari/537.36'} \n",
    "    req = urllib.request.Request(url=url, headers=headers) \n",
    "    page = urllib.request.urlopen(req)\n",
    "#     html = page.read().decode('gb2312','ignore')\n",
    "    html = page.read().decode('gb2312')\n",
    "    page.close()\n",
    "    return html\n",
    "\n",
    "raw_url = 'http://zkxb.xjtu.edu.cn/oa/scriptlsit.aspx?kind=Issue&issnum=%B5%DA12%C6%DA&year=2020%C4%EA'\n",
    "html = getHtml(raw_url)\n",
    "# print(html)\n",
    "bs = BeautifulSoup(html, 'html.parser') #解析网页\n",
    "hyperlink = bs.find_all('a')  #获取所有超链接\n",
    "# print(hyperlink)\n",
    "file = open('demo.txt', 'w')\n",
    " \n",
    "for h in hyperlink:\n",
    "    hh = h.get('href')\n",
    "    if hh and 'Sid=' in hh and 'pdff' not in hh:  #筛选博客链接\n",
    "        print(hh)\n",
    "        file.write(hh)   #写入到“demo.txt”文件中\n",
    "        file.write('\\n')\n",
    "        \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#统计每个关键词出现的次数\n",
    "import collections\n",
    "import os\n",
    "with open('blog_key.txt') as file1:#打开文本文件\n",
    "str1=file1.read().split(' ')#将文章按照空格划分开\n",
    "print \"原文本:\\n %s\"% str1\n",
    "print \"\\n各单词出现的次数：\\n %s\" % collections.Counter(str1)\n",
    "print collections.Counter(str1)['a']#以字典的形式存储，每个字符对应的键值就是在文本中出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制词频云图\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt #数据可视化\n",
    "import jieba #词语切割\n",
    "import wordcloud #分词\n",
    "from wordcloud import WordCloud,ImageColorGenerator,STOPWORDS #词云，颜色生成器，停止\n",
    "import numpy as np #科学计算\n",
    "from PIL import Image #处理图片\n",
    "\n",
    "def ciyun():\n",
    "    #打开文本\n",
    "    with open('blog_key.txt','r',encoding='UTF-8') as f:  # 打开新的文本转码为utf-8\n",
    "        textfile= f.read()  #读取文本内容\n",
    "    wordlist = jieba.cut_for_search(textfile)#切割词语\n",
    "    space_list = ' '.join(wordlist) # 链接词语\n",
    "    backgroud = np.array(Image.open('beijin.png')) #背景图片，只有黑白图才能按照形状生成词云\n",
    "    mywordcloud = WordCloud(width=14000, height=2200,\n",
    "                            background_color=None, mode=\"RGBA\",#背景颜色\n",
    "                            mask=backgroud, #写字用的背景图，从图片中提取颜色\n",
    "                            max_words=500, #最大词语数\n",
    "                            stopwords=STOPWORDS,#停止的默认词语\n",
    "                            font_path='simkai.ttf',#源码自带字体\n",
    "                            max_font_size=200,#最大字体尺寸\n",
    "                            random_state=50,#随机角度\n",
    "                            scale=1).generate(space_list) #生成词云\n",
    "    image_color = ImageColorGenerator(backgroud)#生成词云的颜色\n",
    "    plt.imshow(mywordcloud) #显示词云\n",
    "    plt.axis('off') #关闭坐标（x,y轴）\n",
    "    plt.savefig('cytu.png') #保存图片\n",
    "    plt.show()#显示\n",
    "\n",
    "def main():\n",
    "    ciyun()\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
