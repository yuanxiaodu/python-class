{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gyh(data):\n",
    "    s=(data-data.min())/(data.max()-data.min())\n",
    "    return s\n",
    "\n",
    "def ycl(path,label):\n",
    "    data=np.loadtxt(path)\n",
    "\n",
    "    x_train=[]\n",
    "    x_test=[]\n",
    "    \n",
    "    for i in range(10):\n",
    "        data_new=data[0+i*100:102400+i*100]\n",
    "        data_new=data_new.reshape(50,2048)\n",
    "\n",
    "        np.random.shuffle(data_new)\n",
    "        train=gyh(data_new[:40,:])\n",
    "        test=gyh(data_new[40:,:])\n",
    "        x_train.append(train)\n",
    "        x_test.append(test)\n",
    "    \n",
    "    x_train=np.array(x_train).reshape(-1,2048)\n",
    "    x_test=np.array(x_test).reshape(-1,2048)\n",
    "    y_train=np.array([label for i in range(0,400)])\n",
    "    y_test=np.array([label for i in range(0,100)])\n",
    "    \n",
    "    return x_train,y_train,x_test,y_test\n",
    "def stackkk(a,b,c,d,e,f,g,h):\n",
    "    aa=np.vstack((a,e))\n",
    "    bb=np.hstack((b,f))\n",
    "    cc=np.vstack((c,g))\n",
    "    dd=np.hstack((d,h))\n",
    "    return aa,bb,cc,dd\n",
    "def dataset():\n",
    "    '''\n",
    "    0:正常轴承\n",
    "    1：内圈受损0.007\n",
    "    2：滚珠受损0.007\n",
    "    3：外圈受损0.007\n",
    "    4: 内圈受损0.014\n",
    "    5: 滚珠受损0.014\n",
    "    6: 外圈受损0.014\n",
    "    7: 内圈受损0.021\n",
    "    8: 滚珠受损0.021\n",
    "    9: 外圈受损0.021\n",
    "    \n",
    "    '''\n",
    "    x_train_1=[]\n",
    "    y_train_1=[]\n",
    "    x_test_1=[]\n",
    "    y_test_1=[]\n",
    "    x_train0,y_train0,x_test0,y_test0=ycl('./12K_21_2/Normal.txt',0)\n",
    "\n",
    "    x_train1,y_train1,x_test1,y_test1=ycl('./12K_21_2/IR07.txt',1)\n",
    "    x_train2,y_train2,x_test2,y_test2=ycl('./12K_21_2/B07.txt',2) \n",
    "    x_train3,y_train3,x_test3,y_test3=ycl('./12K_21_2/OR07.txt',3)\n",
    "    \n",
    "    x_train4,y_train4,x_test4,y_test4=ycl('./12K_21_2/IR14.txt',4)\n",
    "    x_train5,y_train5,x_test5,y_test5=ycl('./12K_21_2/B14.txt',5) \n",
    "    x_train6,y_train6,x_test6,y_test6=ycl('./12K_21_2/OR14.txt',6)\n",
    "    \n",
    "    x_train7,y_train7,x_test7,y_test7=ycl('./12K_21_2/IR21.txt',7)\n",
    "    x_train8,y_train8,x_test8,y_test8=ycl('./12K_21_2/B21.txt',8) \n",
    "    x_train9,y_train9,x_test9,y_test9=ycl('./12K_21_2/OR21.txt',9)\n",
    "    \n",
    "    x_train_1.extend([x_train0,x_train1,x_train2,x_train3,x_train4,x_train5,x_train6,x_train7,x_train8,x_train9])\n",
    "    y_train_1.extend([y_train0,y_train1,y_train2,y_train3,y_train4,y_train5,y_train6,y_train7,y_train8,y_train9])\n",
    "    x_test_1.extend([x_test0,x_test1,x_test2,x_test3,x_test4,x_test5,x_test6,x_test7,x_test8,x_test9])\n",
    "    y_test_1.extend([y_test0,y_test1,y_test2,y_test3,y_test4,y_test5,y_test6,y_test7,y_test8,y_test9])\n",
    "\n",
    "    \n",
    "    a,b,c,d=x_train_1[0],y_train_1[0],x_test_1[0],y_test_1[0]\n",
    "    for i in range(1,10,1):\n",
    "        \n",
    "        a,b,c,d=stackkk(a,b,c,d,x_train_1[i],y_train_1[i],x_test_1[i],y_test_1[i])\n",
    "\n",
    "    \n",
    "    return a,b,c,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test=dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_=x_train.reshape(-1,2048,1)\n",
    "x_test_=x_test.reshape(-1,2048,1)\n",
    "y_train_=utils.to_categorical(y_train,num_classes=10)\n",
    "y_test_=utils.to_categorical(y_test,num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 2048, 1) (4000, 10) (1000, 2048, 1) (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_.shape,y_train_.shape,x_test_.shape,y_test_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据与标签构造为一个元组\n",
    "train_db=tf.data.Dataset.from_tensor_slices((x_train_,y_train_))\n",
    "shuffle_db=train_db.shuffle(1000)# 打乱数据\n",
    "train_data=shuffle_db.batch(100)# 分组\n",
    "\n",
    "test_db=tf.data.Dataset.from_tensor_slices((x_test_,y_test_))\n",
    "test_data=test_db.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model, Sequential\n",
    "\n",
    "# 18层，34层的网络残差模块\n",
    "class BasicBlock(layers.Layer):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, out_channel, strides=1, downsample=None, **kwargs):\n",
    "        super(BasicBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv1D(out_channel, kernel_size=3, strides=strides,\n",
    "                                   padding=\"SAME\", use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = layers.Conv1D(out_channel, kernel_size=3, strides=1,\n",
    "                                   padding=\"SAME\", use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "        # -----------------------------------------\n",
    "        self.downsample = downsample\n",
    "        self.relu = layers.ReLU()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        identity = inputs\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        x = self.add([identity, x])\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 针对50，101，152层网络的残差模块\n",
    "class Bottleneck(layers.Layer):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, out_channel, strides=1, downsample=None, **kwargs):\n",
    "        super(Bottleneck, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv1D(out_channel, kernel_size=1, use_bias=False, name=\"conv1\")\n",
    "        self.bn1 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv1/BatchNorm\")\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = layers.Conv1D(out_channel, kernel_size=3, use_bias=False,\n",
    "                                   strides=strides, padding=\"SAME\", name=\"conv2\")\n",
    "        self.bn2 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv2/BatchNorm\")\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = layers.Conv1D(out_channel * self.expansion, kernel_size=1, use_bias=False, name=\"conv3\")\n",
    "        self.bn3 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv3/BatchNorm\")\n",
    "        # -----------------------------------------\n",
    "        self.relu = layers.ReLU()\n",
    "        self.downsample = downsample\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        identity = inputs\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        x = self.add([x, identity])\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def _make_layer(block, in_channel, channel, block_num, name, strides=1):\n",
    "    downsample = None\n",
    "    if strides != 1 or in_channel != channel * block.expansion:\n",
    "        downsample = Sequential([\n",
    "            layers.Conv1D(channel * block.expansion, kernel_size=1, strides=strides,\n",
    "                          use_bias=False, name=\"conv1\"),\n",
    "            layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5, name=\"BatchNorm\")\n",
    "        ], name=\"shortcut\")\n",
    "\n",
    "    layers_list = []\n",
    "    layers_list.append(block(channel, downsample=downsample, strides=strides, name=\"unit_1\"))\n",
    "\n",
    "    for index in range(1, block_num):\n",
    "        layers_list.append(block(channel, name=\"unit_\" + str(index + 1)))\n",
    "\n",
    "    return Sequential(layers_list, name=name)\n",
    "\n",
    "\n",
    "\n",
    "def _resnet(block, blocks_num, im_len=2048, num_classes=1000, include_top=True):\n",
    "    #block:残差模块，blocks_num,残差结构的个数\n",
    "    # tensorflow中的tensor通道排序是NHWC\n",
    "    # (None, 224, 224, 3)\n",
    "    input_data = layers.Input(shape=(im_len,1), dtype=\"float32\")\n",
    "    x = layers.Conv1D(filters=64, kernel_size=7, strides=2,padding=\"SAME\", use_bias=False, name=\"conv1\")(input_data)\n",
    "    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv1/BatchNorm\")(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool1D(pool_size=3, strides=2, padding=\"SAME\")(x)\n",
    "\n",
    "    x = _make_layer(block, x.shape[-1], 64, blocks_num[0], name=\"block1\")(x)\n",
    "    x = _make_layer(block, x.shape[-1], 128, blocks_num[1], strides=2, name=\"block2\")(x)\n",
    "    x = _make_layer(block, x.shape[-1], 256, blocks_num[2], strides=2, name=\"block3\")(x)\n",
    "    x = _make_layer(block, x.shape[-1], 512, blocks_num[3], strides=2, name=\"block4\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAvgPool1D()(x)  # pool + flatten\n",
    "        x = layers.Dense(num_classes, name=\"logits\")(x)\n",
    "        predict = layers.Softmax()(x)#最后的层数为1000\n",
    "    else:\n",
    "        predict = x\n",
    "\n",
    "    model = Model(inputs=input_data, outputs=predict)\n",
    "\n",
    "    return model\n",
    "\n",
    "def resnet18(im_len=2048, num_classes=1000, include_top=True):\n",
    "    return _resnet(BasicBlock, [2, 2, 2, 2], im_len, num_classes, include_top)\n",
    "\n",
    "def resnet34(im_len=2048, num_classes=1000, include_top=True):\n",
    "    return _resnet(BasicBlock, [3, 4, 6, 3], im_len, num_classes, include_top)\n",
    "\n",
    "\n",
    "def resnet50(im_len=2048, num_classes=1000, include_top=True):\n",
    "    return _resnet(Bottleneck, [3, 4, 6, 3], im_len,num_classes, include_top)\n",
    "\n",
    "\n",
    "def resnet101(im_len=2048, num_classes=1000, include_top=True):\n",
    "    return _resnet(Bottleneck, [3, 4, 23, 3], im_len, num_classes, include_top)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "functional_5 (Functional)    (None, 64, 512)           3853504   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 4,908,746\n",
      "Trainable params: 4,899,146\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet=resnet18(include_top=False)\n",
    "model=tf.keras.Sequential([\n",
    "    resnet,\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    layers.Dense(1024,activation='relu'),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    layers.Dense(512,activation='relu'),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    layers.Dense(10,activation='softmax')\n",
    "    \n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    metrics=[keras.metrics.categorical_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 139s 3s/step - loss: 0.2391 - categorical_accuracy: 0.8963 - val_loss: 0.1595 - val_categorical_accuracy: 0.9370\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 141s 4s/step - loss: 0.1201 - categorical_accuracy: 0.9442 - val_loss: 0.0853 - val_categorical_accuracy: 0.9790\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 151s 4s/step - loss: 0.0448 - categorical_accuracy: 0.9890 - val_loss: 0.0527 - val_categorical_accuracy: 0.9890\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 164s 4s/step - loss: 0.0286 - categorical_accuracy: 0.9918 - val_loss: 0.0715 - val_categorical_accuracy: 0.9890\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 177s 4s/step - loss: 0.0121 - categorical_accuracy: 0.9962 - val_loss: 0.0649 - val_categorical_accuracy: 0.9900\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 173s 4s/step - loss: 0.0116 - categorical_accuracy: 0.9970 - val_loss: 0.0304 - val_categorical_accuracy: 0.9900\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 176s 4s/step - loss: 0.0143 - categorical_accuracy: 0.9967 - val_loss: 0.0759 - val_categorical_accuracy: 0.9890\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 173s 4s/step - loss: 0.0093 - categorical_accuracy: 0.9973 - val_loss: 0.0401 - val_categorical_accuracy: 0.9920\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 170s 4s/step - loss: 0.0106 - categorical_accuracy: 0.9960 - val_loss: 0.0129 - val_categorical_accuracy: 0.9910\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 170s 4s/step - loss: 0.0221 - categorical_accuracy: 0.9925 - val_loss: 0.3885 - val_categorical_accuracy: 0.8990\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train_,y_train_,batch_size=100,epochs=10,validation_data=(x_test_,y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 171s 4s/step - loss: 0.0137 - categorical_accuracy: 0.9958 - val_loss: 0.0347 - val_categorical_accuracy: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x188a119a5e0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_,y_train_,batch_size=100,epochs=1,validation_data=(x_test_,y_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.996>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model.predict(x_test_[0:500]).argmax(axis=1)\n",
    "acc=tf.reduce_mean(tf.cast(tf.equal(pred,y_test[0:500]),tf.float32))\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1b500016970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Model, Sequential\n",
    "\n",
    "\n",
    "def VGG(feature, im_height=224, im_width=224, class_num=1000):\n",
    "    # tensorflow中的tensor通道排序是NHWC\n",
    "    input_image = layers.Input(shape=(im_height, im_width, 3), dtype=\"float32\")\n",
    "    x = feature(input_image)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(rate=0.5)(x)\n",
    "    x = layers.Dense(2048, activation='relu')(x)\n",
    "    x = layers.Dropout(rate=0.5)(x)\n",
    "    x = layers.Dense(2048, activation='relu')(x)\n",
    "    x = layers.Dense(class_num)(x)\n",
    "    output = layers.Softmax()(x)\n",
    "    model = models.Model(inputs=input_image, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def features(cfg):\n",
    "    feature_layers = []\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            feature_layers.append(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "        else:\n",
    "            conv2d = layers.Conv2D(v, kernel_size=3, padding=\"SAME\", activation=\"relu\")\n",
    "            feature_layers.append(conv2d)\n",
    "    return Sequential(feature_layers, name=\"feature\")\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def vgg(model_name=\"vgg16\", im_height=224, im_width=224, class_num=1000):\n",
    "    try:\n",
    "        cfg = cfgs[model_name]\n",
    "    except:\n",
    "        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n",
    "        exit(-1)\n",
    "    model = VGG(features(cfg), im_height=im_height, im_width=im_width, class_num=class_num)\n",
    "    return model\n",
    "\n",
    "vgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "cannot find D:\\jupyter\\data_set\\flower_data\\train",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-4326c957fe70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-4326c957fe70>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtrain_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mvalidation_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"val\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cannot find {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cannot find {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: cannot find D:\\jupyter\\data_set\\flower_data\\train"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
    "    image_path = os.path.join(data_root, \"data_set\", \"flower_data\")  # flower data set path\n",
    "    train_dir = os.path.join(image_path, \"train\")\n",
    "    validation_dir = os.path.join(image_path, \"val\")\n",
    "    assert os.path.exists(train_dir), \"cannot find {}\".format(train_dir)\n",
    "    assert os.path.exists(validation_dir), \"cannot find {}\".format(validation_dir)\n",
    "\n",
    "    # create direction for saving weights\n",
    "    if not os.path.exists(\"save_weights\"):\n",
    "        os.makedirs(\"save_weights\")\n",
    "\n",
    "    im_height = 224\n",
    "    im_width = 224\n",
    "    batch_size = 32\n",
    "    epochs = 10\n",
    "\n",
    "    _R_MEAN = 123.68\n",
    "    _G_MEAN = 116.78\n",
    "    _B_MEAN = 103.94\n",
    "\n",
    "    def pre_function(img):\n",
    "        # img = im.open('test.jpg')\n",
    "        # img = np.array(img).astype(np.float32)\n",
    "        img = img - [_R_MEAN, _G_MEAN, _B_MEAN]\n",
    "\n",
    "        return img\n",
    "\n",
    "    # data generator with data augmentation\n",
    "    train_image_generator = ImageDataGenerator(horizontal_flip=True,\n",
    "                                               preprocessing_function=pre_function)\n",
    "    validation_image_generator = ImageDataGenerator(preprocessing_function=pre_function)\n",
    "\n",
    "    train_data_gen = train_image_generator.flow_from_directory(directory=train_dir,\n",
    "                                                               batch_size=batch_size,\n",
    "                                                               shuffle=True,\n",
    "                                                               target_size=(im_height, im_width),\n",
    "                                                               class_mode='categorical')\n",
    "    total_train = train_data_gen.n\n",
    "\n",
    "    # get class dict\n",
    "    class_indices = train_data_gen.class_indices\n",
    "\n",
    "    # transform value and key of dict\n",
    "    inverse_dict = dict((val, key) for key, val in class_indices.items())\n",
    "    # write dict into json file\n",
    "    json_str = json.dumps(inverse_dict, indent=4)\n",
    "    with open('class_indices.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    val_data_gen = validation_image_generator.flow_from_directory(directory=validation_dir,\n",
    "                                                                  batch_size=batch_size,\n",
    "                                                                  shuffle=False,\n",
    "                                                                  target_size=(im_height, im_width),\n",
    "                                                                  class_mode='categorical')\n",
    "    total_val = val_data_gen.n\n",
    "    print(\"using {} images for training, {} images for validation.\".format(total_train,\n",
    "                                                                           total_val))\n",
    "\n",
    "    model = vgg(\"vgg16\", 224, 224, 5)\n",
    "\n",
    "    pre_weights_path = './pretrain_weights.ckpt'\n",
    "    assert len(glob.glob(pre_weights_path+\"*\")), \"cannot find {}\".format(pre_weights_path)\n",
    "    model.load_weights(pre_weights_path)\n",
    "    for layer_t in model.layers:\n",
    "        if layer_t.name == 'feature':\n",
    "            layer_t.trainable = False\n",
    "            break\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # using keras high level api for training\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "    callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='./save_weights/myAlex_{epoch}.h5',\n",
    "                                                    save_best_only=True,\n",
    "                                                    save_weights_only=True,\n",
    "                                                    monitor='val_loss')]\n",
    "\n",
    "    # tensorflow2.1 recommend to using fit\n",
    "    history = model.fit(x=train_data_gen,\n",
    "                        steps_per_epoch=total_train // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_data_gen,\n",
    "                        validation_steps=total_val // batch_size,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
