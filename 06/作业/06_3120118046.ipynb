{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itemadapter import ItemAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieCommentPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        if not item.get('star'):\n",
    "            item['star'] = 0\n",
    "        elif item['star'] == '力荐':\n",
    "            item['star'] = 5\n",
    "        elif item['star'] == '推荐':\n",
    "            item['star'] = 4\n",
    "        elif item['star'] == '还行':\n",
    "            item['star'] = 3\n",
    "        elif item['star'] == '较差':\n",
    "            item['star'] = 2\n",
    "        elif item['star'] == '很差':\n",
    "            item['star'] = 1\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Request\n",
    "from scrapy.http import Response\n",
    "from scrapy.loader import ItemLoader\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in d:\\anaconda3\\lib\\site-packages (from selenium) (1.25.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy_selenium in d:\\anaconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: selenium>=3.9.0 in d:\\anaconda3\\lib\\site-packages (from scrapy_selenium) (3.141.0)\n",
      "Requirement already satisfied: scrapy>=1.0.0 in d:\\anaconda3\\lib\\site-packages (from scrapy_selenium) (2.4.1)\n",
      "Requirement already satisfied: urllib3 in d:\\anaconda3\\lib\\site-packages (from selenium>=3.9.0->scrapy_selenium) (1.25.9)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (18.1.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (0.2.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (19.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (1.22.0)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (4.5.2)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (1.0.4)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (20.3.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (2.0.5)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (1.5.0)\n",
      "Requirement already satisfied: protego>=0.1.15 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (0.1.16)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (1.1.0)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (4.7.1)\n",
      "Requirement already satisfied: parsel>=1.5.0 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (1.6.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in d:\\anaconda3\\lib\\site-packages (from scrapy>=1.0.0->scrapy_selenium) (2.9.2)\n",
      "Requirement already satisfied: pyasn1 in d:\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy>=1.0.0->scrapy_selenium) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in d:\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy>=1.0.0->scrapy_selenium) (0.2.8)\n",
      "Requirement already satisfied: attrs>=16.0.0 in d:\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy>=1.0.0->scrapy_selenium) (19.3.0)\n",
      "Requirement already satisfied: six>=1.5.2 in d:\\anaconda3\\lib\\site-packages (from pyOpenSSL>=16.2.0->scrapy>=1.0.0->scrapy_selenium) (1.15.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in d:\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy>=1.0.0->scrapy_selenium) (0.10.0)\n",
      "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in d:\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy>=1.0.0->scrapy_selenium) (2.0.2)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in d:\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy>=1.0.0->scrapy_selenium) (20.0.1)\n",
      "Requirement already satisfied: Automat>=0.3.0 in d:\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy>=1.0.0->scrapy_selenium) (20.2.0)\n",
      "Requirement already satisfied: incremental>=16.10.1 in d:\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy>=1.0.0->scrapy_selenium) (17.5.0)\n",
      "Requirement already satisfied: constantly>=15.1 in d:\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy>=1.0.0->scrapy_selenium) (15.1.0)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda3\\lib\\site-packages (from zope.interface>=4.1.3->scrapy>=1.0.0->scrapy_selenium) (49.2.0.post20200714)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in d:\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy>=1.0.0->scrapy_selenium) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.5 in d:\\anaconda3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy>=1.0.0->scrapy_selenium) (2.10)\n",
      "Requirement already satisfied: pycparser in d:\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy>=1.0.0->scrapy_selenium) (2.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy_selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy_selenium import SeleniumRequest\n",
    "from selenium.common.exceptions import NoSuchElementException, NoSuchFrameException\n",
    "from selenium.webdriver import Chrome, ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.4.0.46-cp38-cp38-win_amd64.whl (33.5 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\anaconda3\\lib\\site-packages (from opencv-python) (1.18.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.4.0.46\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import random\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.item import Item, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieCommentItem(Item):\n",
    "    name = Field()\n",
    "    seen = Field()\n",
    "    star = Field()\n",
    "    date = Field()\n",
    "    votes = Field()\n",
    "    text = Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-2781905c7db1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMovieCommentItem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from ..items import MovieCommentItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieCommentSpider(scrapy.Spider):\n",
    "    name = \"movie_comment\"\n",
    "    allowed_domains = ['douban.com']\n",
    "    cookies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, start=1, end=1, sort='new_score', **kwargs):\n",
    "        \"\"\"\n",
    "        :param start: 起始页（包含）\n",
    "        :param end: 结束页（包含）\n",
    "        :param sort: # 'new_score'（热门） or 'time'（最新）\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "        self.end = int(end)\n",
    "        self.base_url = f'https://movie.douban.com/subject/{kwargs[\"id\"]}/comments?start={(int(start) - 1) * 20}&limit=20&status=P&sort={sort}'\n",
    "        with open('../../douban_accounts.json', encoding='UTF-8') as f:\n",
    "            js = json.load(f)\n",
    "            self.accounts = js['accounts']\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_requests(self):\n",
    "        for account in self.accounts:\n",
    "            yield SeleniumRequest(url='https://www.douban.com/', wait_time=10, callback=self.login,\n",
    "                                  meta={'name': account['name'], 'password': account['password']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def save_img(bk_block):\n",
    "        \"\"\"保存图片\"\"\"\n",
    "        try:\n",
    "            img = requests.get(bk_block).content\n",
    "            with open('bg.jpeg', 'wb') as f:\n",
    "                f.write(img)\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def get_pos():\n",
    "        \"\"\"识别缺口\n",
    "        注意：网页上显示的图片为缩放图片，缩放 50% 所以识别坐标需要 0.5\n",
    "        \"\"\"\n",
    "        image = cv.imread('bg.jpeg')\n",
    "        blurred = cv.GaussianBlur(image, (5, 5), 0)\n",
    "        canny = cv.Canny(blurred, 200, 400)\n",
    "        contours, hierarchy = cv.findContours(canny, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        for i, contour in enumerate(contours):\n",
    "            m = cv.moments(contour)\n",
    "            if m['m00'] == 0:\n",
    "                cx = cy = 0\n",
    "            else:\n",
    "                cx, cy = m['m10'] / m['m00'], m['m01'] / m['m00']\n",
    "            if 6000 < cv.contourArea(contour) < 8000 and 370 < cv.arcLength(contour, True) < 390:\n",
    "                if cx < 400:\n",
    "                    continue\n",
    "                x, y, w, h = cv.boundingRect(contour)  # 外接矩形\n",
    "                cv.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "                cv.imshow('image', image)  # 显示识别结果\n",
    "                print(f'【缺口识别】 {x / 2} px')\n",
    "                return x / 2\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def get_track(distance):\n",
    "        \"\"\"模拟轨迹\n",
    "        \"\"\"\n",
    "        distance -= 68  # 初始位置\n",
    "        # 初速度\n",
    "        v = 0\n",
    "        # 单位时间为0.2s来统计轨迹，轨迹即0.2内的位移\n",
    "        t = 0.2\n",
    "        # 位移/轨迹列表，列表内的一个元素代表0.2s的位移\n",
    "        tracks = []\n",
    "        # 当前的位移\n",
    "        current = 0\n",
    "        # 到达mid值开始减速\n",
    "        mid = distance * 7 / 8\n",
    "\n",
    "        distance += 10  # 先滑过一点，最后再反着滑动回来\n",
    "        # a = random.randint(1,3)\n",
    "        while current < distance:\n",
    "            if current < mid:\n",
    "                # 加速度越小，单位时间的位移越小,模拟的轨迹就越多越详细\n",
    "                a = random.randint(1, 2)  # 加速运动\n",
    "            else:\n",
    "                a = -random.randint(1, 3)  # 减速运动\n",
    "\n",
    "            # 初速度\n",
    "            v0 = v\n",
    "            # 0.2秒时间内的位移\n",
    "            s = v0 * t + 0.5 * a * (t ** 2)\n",
    "            # 当前的位置\n",
    "            current += s\n",
    "            # 添加到轨迹列表\n",
    "            tracks.append(round(s))\n",
    "\n",
    "            # 速度已经达到v,该速度作为下次的初速度\n",
    "            v = v0 + a * t\n",
    "\n",
    "        # 反着滑动到大概准确位置\n",
    "        for i in range(4):\n",
    "            tracks.append(-random.randint(2, 3))\n",
    "        for i in range(4):\n",
    "            tracks.append(-random.randint(1, 3))\n",
    "        return tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def ease_out_quart(x):\n",
    "        return 1 - pow(1 - x, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    @staticmethod\n",
    "    def get_tracks_2(distance, seconds, ease_func):\n",
    "        \"\"\"\n",
    "        根据轨迹离散分布生成的数学 生成  # 参考文档  https://www.jianshu.com/p/3f968958af5a\n",
    "        成功率很高 90% 往上\n",
    "        :param distance: 缺口位置\n",
    "        :param seconds:  时间\n",
    "        :param ease_func: 生成函数\n",
    "        :return: 轨迹数组\n",
    "        \"\"\"\n",
    "        distance -= 68  # 初始位置\n",
    "        distance += 20\n",
    "        tracks = [0]\n",
    "        offsets = [0]\n",
    "        for t in np.arange(0.0, seconds, 0.1):\n",
    "            ease = ease_func\n",
    "            offset = round(ease(t / seconds) * distance)\n",
    "            tracks.append(offset - offsets[-1])\n",
    "            offsets.append(offset)\n",
    "        tracks.extend([-3, -2, -3, -2, -2, -2, -2, -1, -0, -1, -1, -1])\n",
    "        return tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login(self, response):\n",
    "        browser: Chrome = response.request.meta['driver']\n",
    "        browser.delete_all_cookies()\n",
    "        time.sleep(0.5)\n",
    "        browser.switch_to.frame(0)\n",
    "        browser.find_element_by_xpath('/html/body/div[1]/div[1]/ul[1]/li[2]').click()\n",
    "        browser.find_element_by_xpath('//*[@id=\"username\"]').send_keys(response.request.meta['name'])\n",
    "        browser.find_element_by_xpath('//*[@id=\"password\"]').send_keys(response.request.meta['password'])\n",
    "        browser.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div[5]/a').click()\n",
    "        time.sleep(2)\n",
    "        cookie = browser.get_cookie('dbcl2')\n",
    "        if not cookie:\n",
    "            try:\n",
    "                WebDriverWait(browser, 10, 0.5).until(\n",
    "                    EC.presence_of_element_located((By.ID, 'tcaptcha_iframe')))  # 等待 iframe\n",
    "                browser.switch_to.frame(browser.find_element_by_id('tcaptcha_iframe'))  # 加载 iframe\n",
    "                time.sleep(0.5)\n",
    "            except NoSuchFrameException:\n",
    "                print('No reCAPTCHA NoSuchFrameException')\n",
    "            except NoSuchElementException:\n",
    "                print('No reCAPTCHA NoSuchElementException')\n",
    "            else:\n",
    "                bk_block = browser.find_element_by_xpath('//*[@id=\"slideBg\"]').get_attribute('src')\n",
    "                if self.save_img(bk_block):\n",
    "                    dex = self.get_pos()\n",
    "                    track_list = self.get_track(dex)\n",
    "                    track_list = self.get_tracks_2(dex, 3, self.ease_out_quart)\n",
    "                    time.sleep(0.52)\n",
    "                    slid_ing = browser.find_element_by_xpath('//div[@id=\"tcaptcha_drag_thumb\"]')  # 滑块定位\n",
    "                    ActionChains(browser).click_and_hold(on_element=slid_ing).perform()  # 鼠标按下\n",
    "                    time.sleep(0.21)\n",
    "                    print('轨迹', track_list)\n",
    "                    for track in track_list:\n",
    "                        ActionChains(browser).move_by_offset(xoffset=track, yoffset=0).perform()  # 鼠标移动到距离当前位置（x,y）\n",
    "                    time.sleep(0.441)\n",
    "                    ActionChains(browser).release(on_element=slid_ing).perform()  # print('第三步,释放鼠标')\n",
    "                    # 识别图片\n",
    "                    time.sleep(2)\n",
    "                    cookie = browser.get_cookie('dbcl2')\n",
    "                else:\n",
    "                    print('缺口图片捕获失败')\n",
    "        browser.quit()\n",
    "        if cookie:\n",
    "            self.cookies.append({'dbcl2': eval(cookie['value'])})\n",
    "            if len(self.cookies) >= len(self.accounts) / 2:\n",
    "                print('登录完成，开始爬取...')\n",
    "                yield Request(url=self.base_url,\n",
    "                              meta={'cookiejar': 1, 'dont_redirect': True, 'handle_httpstatus_list': [302]},\n",
    "                              callback=self.parse, dont_filter=True)\n",
    "        else:\n",
    "            print(f'用户 {response.request.meta[\"name\"]} 登录失败')\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response, **kwargs):\n",
    "        for comment in response.css('div.comment-item'):\n",
    "            item = MovieCommentItem()\n",
    "            item['name'] = comment.xpath('div[2]/h3/span[2]/a/text()').get()\n",
    "            item['seen'] = comment.xpath('div[2]/h3/span[2]/span[1]/text()').get()\n",
    "            item['star'] = comment.xpath('div[2]/h3/span[2]/span[has-class(\"rating\")]/@title').get()\n",
    "            item['date'] = comment.xpath('div[2]/h3/span[2]/span[has-class(\"comment-time\")]/@title').get()\n",
    "            item['votes'] = comment.xpath('div[2]/h3/span[1]/span/text()').get()\n",
    "            item['text'] = comment.xpath('div[2]/p/span/text()').get()\n",
    "            yield item\n",
    "        next_page = response.css('a.next::attr(\"href\")')\n",
    "        if next_page.get() is not None:\n",
    "            start = next_page.re_first(r'\\?start=(\\d+)')\n",
    "            if self.end == -1 or self.end > int(start) / 20:\n",
    "                yield response.follow(next_page.get(), self.parse, dont_filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from scrapy import signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubanSpiderMiddleware:  \n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
